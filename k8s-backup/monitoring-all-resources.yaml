apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: alertmanager
    creationTimestamp: "2025-12-27T11:11:55Z"
    generateName: alertmanager-obs-kube-prometheus-stack-alertmanager-
    generation: 1
    labels:
      alertmanager: obs-kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: obs-kube-prometheus-stack-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.30.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: alertmanager-obs-kube-prometheus-stack-alertmanager-5b5bf7994b
      statefulset.kubernetes.io/pod-name: alertmanager-obs-kube-prometheus-stack-alertmanager-0
    name: alertmanager-obs-kube-prometheus-stack-alertmanager-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-obs-kube-prometheus-stack-alertmanager
      uid: 685cd12b-b2f3-43d4-a02f-689122153bba
    resourceVersion: "741549"
    uid: e60af310-6471-49c9-8619-81911670dc6c
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - alertmanager
              - key: alertmanager
                operator: In
                values:
                - obs-kube-prometheus-stack-alertmanager
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://obs-kube-prometheus-stack-alertmanager.monitoring:9093
      - --web.route-prefix=/
      - --cluster.label=monitoring/obs-kube-prometheus-stack-alertmanager
      - --cluster.peer=alertmanager-obs-kube-prometheus-stack-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.30.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-obs-kube-prometheus-stack-alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
        name: cluster-tls-config
        readOnly: true
        subPath: cluster-tls-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xntlb
        readOnly: true
    - args:
      - --listen-address=:8080
      - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
      - --reload-url=http://127.0.0.1:9093/-/reload
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xntlb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-obs-kube-prometheus-stack-alertmanager-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-init
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xntlb
        readOnly: true
    nodeName: aks-nodepool1-41991914-vmss000000
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: obs-kube-prometheus-stack-alertmanager
    serviceAccountName: obs-kube-prometheus-stack-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-obs-kube-prometheus-stack-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-obs-kube-prometheus-stack-alertmanager-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - name: web-config
      secret:
        defaultMode: 420
        secretName: alertmanager-obs-kube-prometheus-stack-alertmanager-web-config
    - name: cluster-tls-config
      secret:
        defaultMode: 420
        secretName: alertmanager-obs-kube-prometheus-stack-alertmanager-cluster-tls-config
    - emptyDir: {}
      name: alertmanager-obs-kube-prometheus-stack-alertmanager-db
    - name: kube-api-access-xntlb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:58Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:12:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:12:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        memory: 200Mi
      containerID: containerd://ed31a5f3003f3d610165d42c27c2db0b324b541af5043cd1001455ea99ff3fe7
      image: quay.io/prometheus/alertmanager:v0.30.0
      imageID: quay.io/prometheus/alertmanager@sha256:abb750ac7b63116761c16dd481ae92496fbe04721686c0920f0fa4d0728cd4a6
      lastState: {}
      name: alertmanager
      ready: true
      resources:
        requests:
          memory: 200Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:12:01Z"
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /alertmanager
        name: alertmanager-obs-kube-prometheus-stack-alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
        name: cluster-tls-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xntlb
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://8171f50c24aaa74cf78dfaca88dc22aaca53352994131d1d40318e1336ed3808
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:257ce3543e8f45c059dab488bcf897d8cc598cabd78afba12f41d6dbe36e22d2
      lastState: {}
      name: config-reloader
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:12:01Z"
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xntlb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.224.0.4
    hostIPs:
    - ip: 10.224.0.4
    initContainerStatuses:
    - containerID: containerd://541b8acdfef827d2e6c7c597ed82b8d0e290fdbfc281775568b2fc996783878d
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:257ce3543e8f45c059dab488bcf897d8cc598cabd78afba12f41d6dbe36e22d2
      lastState: {}
      name: init-config-reloader
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://541b8acdfef827d2e6c7c597ed82b8d0e290fdbfc281775568b2fc996783878d
          exitCode: 0
          finishedAt: "2025-12-27T11:11:57Z"
          reason: Completed
          startedAt: "2025-12-27T11:11:57Z"
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xntlb
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.244.0.124
    podIPs:
    - ip: 10.244.0.124
    qosClass: Burstable
    startTime: "2025-12-27T11:11:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
      checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
      checksum/secret: 481413ea19473fd6217e3a5bcbd4fe4215ddf5da919e91a3e2d32b793cbd8f8d
      kubectl.kubernetes.io/default-container: grafana
    creationTimestamp: "2025-12-27T11:11:50Z"
    generateName: obs-grafana-6d4d847f7c-
    generation: 1
    labels:
      app.kubernetes.io/instance: obs
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.3.0
      helm.sh/chart: grafana-10.4.0
      pod-template-hash: 6d4d847f7c
    name: obs-grafana-6d4d847f7c-s8xnf
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: obs-grafana-6d4d847f7c
      uid: e99c8ca5-4970-492c-874a-cde1c0b5f1b0
    resourceVersion: "742001"
    uid: f8a0ff31-3378-4dbb-9585-65269b71191a
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_dashboard
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /tmp/dashboards
      - name: RESOURCE
        value: both
      - name: NAMESPACE
        value: ALL
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: obs-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: obs-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/dashboards/reload
      - name: REQ_METHOD
        value: POST
      - name: REQ_SKIP_INIT
        value: "true"
      image: quay.io/kiwigrid/k8s-sidecar:2.1.2
      imagePullPolicy: IfNotPresent
      name: grafana-sc-dashboard
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ksx4s
        readOnly: true
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_datasource
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: obs-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: obs-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      - name: REQ_SKIP_INIT
        value: "true"
      image: quay.io/kiwigrid/k8s-sidecar:2.1.2
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ksx4s
        readOnly: true
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: obs-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: obs-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: docker.io/grafana/grafana:12.3.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      - containerPort: 6060
        name: profiling
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
        subPath: provider.yaml
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ksx4s
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: aks-nodepool1-41991914-vmss000001
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: obs-grafana
    serviceAccountName: obs-grafana
    shareProcessNamespace: false
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: obs-grafana
      name: config
    - emptyDir: {}
      name: storage
    - emptyDir: {}
      name: sc-dashboard-volume
    - configMap:
        defaultMode: 420
        name: obs-grafana-config-dashboards
      name: sc-dashboard-provider
    - emptyDir: {}
      name: sc-datasources-volume
    - name: kube-api-access-ksx4s
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:12:38Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:13:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:13:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://95e99283c46095d60ac983c2cc8af1769ad7c94323c40aa29c9192cdb16d88e8
      image: docker.io/grafana/grafana:12.3.0
      imageID: docker.io/grafana/grafana@sha256:70d9599b186ce287be0d2c5ba9a78acb2e86c1a68c9c41449454d0fc3eeb84e8
      lastState: {}
      name: grafana
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:12:37Z"
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ksx4s
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://eebd4f4993a54031dc276ccf4c54fcde8227ae9c458c2dad26f82cec83891746
      image: quay.io/kiwigrid/k8s-sidecar:2.1.2
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:716b0b33ff2dc938a3f2bc64e5ea791d81fb09760bcd27cec1eb896968d6e134
      lastState: {}
      name: grafana-sc-dashboard
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:11:55Z"
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ksx4s
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://60d2b242f15ec01402e2640fb61aa4ec2b6a2aef4838aeeb49d96250625c4efd
      image: quay.io/kiwigrid/k8s-sidecar:2.1.2
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:716b0b33ff2dc938a3f2bc64e5ea791d81fb09760bcd27cec1eb896968d6e134
      lastState: {}
      name: grafana-sc-datasources
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:11:55Z"
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ksx4s
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.224.0.5
    hostIPs:
    - ip: 10.224.0.5
    phase: Running
    podIP: 10.244.1.57
    podIPs:
    - ip: 10.244.1.57
    qosClass: BestEffort
    startTime: "2025-12-27T11:11:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-27T11:11:50Z"
    generateName: obs-kube-prometheus-stack-operator-74bb6546d6-
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.7.0
      chart: kube-prometheus-stack-80.7.0
      heritage: Helm
      pod-template-hash: 74bb6546d6
      release: obs
    name: obs-kube-prometheus-stack-operator-74bb6546d6-qppvl
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: obs-kube-prometheus-stack-operator-74bb6546d6
      uid: 0d33c79d-2863-497f-9869-b502b4e6ce81
    resourceVersion: "741443"
    uid: f00e8ca0-c5e0-4821-9a94-9b718c400def
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --kubelet-service=kube-system/obs-kube-prometheus-stack-kubelet
      - --kubelet-endpoints=true
      - --kubelet-endpointslice=false
      - --localhost=127.0.0.1
      - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      - --config-reloader-cpu-request=0
      - --config-reloader-cpu-limit=0
      - --config-reloader-memory-request=0
      - --config-reloader-memory-limit=0
      - --thanos-default-base-image=quay.io/thanos/thanos:v0.40.1
      - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
      - --web.enable-tls=true
      - --web.cert-file=/cert/cert
      - --web.key-file=/cert/key
      - --web.listen-address=:10250
      - --web.tls-min-version=VersionTLS13
      env:
      - name: GOGC
        value: "30"
      image: quay.io/prometheus-operator/prometheus-operator:v0.87.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: kube-prometheus-stack
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rqth6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: aks-nodepool1-41991914-vmss000000
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: obs-kube-prometheus-stack-operator
    serviceAccountName: obs-kube-prometheus-stack-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tls-secret
      secret:
        defaultMode: 420
        secretName: obs-kube-prometheus-stack-admission
    - name: kube-api-access-rqth6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:55Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d848f5f0227ee5d7b82104cbeef4a21d4e447812a1e82104c80d5299f507bbd2
      image: quay.io/prometheus-operator/prometheus-operator:v0.87.1
      imageID: quay.io/prometheus-operator/prometheus-operator@sha256:6dbbbbeca6d7b94aa30723bfaa55262e41b9f8c15304c484611c696503840aac
      lastState: {}
      name: kube-prometheus-stack
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:11:54Z"
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rqth6
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.224.0.4
    hostIPs:
    - ip: 10.224.0.4
    phase: Running
    podIP: 10.244.0.120
    podIPs:
    - ip: 10.244.0.120
    qosClass: BestEffort
    startTime: "2025-12-27T11:11:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-27T11:11:50Z"
    generateName: obs-kube-state-metrics-6df98f8558-
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-7.0.0
      pod-template-hash: 6df98f8558
      release: obs
    name: obs-kube-state-metrics-6df98f8558-wswrc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: obs-kube-state-metrics-6df98f8558
      uid: b00b5089-ae0d-4173-8381-fb380c513d38
    resourceVersion: "741587"
    uid: bab1ac3a-8a2e-4827-b06a-78137259bdad
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpointslices,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cjxrw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: aks-nodepool1-41991914-vmss000000
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: obs-kube-state-metrics
    serviceAccountName: obs-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-cjxrw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:55Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:12:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:12:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://40386fa4800edbb379741a7729b5b17b827d9658aef32b9ea96b5572f20b11d1
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:2bbc915567334b13632bf62c0a97084aff72a36e13c4dabd5f2f11c898c5bacd
      lastState: {}
      name: kube-state-metrics
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:11:54Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cjxrw
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.224.0.4
    hostIPs:
    - ip: 10.224.0.4
    phase: Running
    podIP: 10.244.0.34
    podIPs:
    - ip: 10.244.0.34
    qosClass: BestEffort
    startTime: "2025-12-27T11:11:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-12-27T11:11:50Z"
    generateName: obs-prometheus-node-exporter-
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      controller-revision-hash: 6c676749db
      helm.sh/chart: prometheus-node-exporter-4.49.2
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: obs
    name: obs-prometheus-node-exporter-25zmp
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: obs-prometheus-node-exporter
      uid: 58785a52-3ec3-4b27-a5e3-723c4b122405
    resourceVersion: "741374"
    uid: 0e42ff50-cb25-42c8-913c-c355bb8a9661
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - aks-nodepool1-41991914-vmss000001
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: aks-nodepool1-41991914-vmss000001
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: obs-prometheus-node-exporter
    serviceAccountName: obs-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:53Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1ba99179f9b2b219c5cff57c38c38b4b76af07bb058c0ecc4b04ccd2f8231bb0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imageID: quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565
      lastState: {}
      name: node-exporter
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:11:53Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.224.0.5
    hostIPs:
    - ip: 10.224.0.5
    phase: Running
    podIP: 10.224.0.5
    podIPs:
    - ip: 10.224.0.5
    qosClass: BestEffort
    startTime: "2025-12-27T11:11:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-12-27T11:11:50Z"
    generateName: obs-prometheus-node-exporter-
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      controller-revision-hash: 6c676749db
      helm.sh/chart: prometheus-node-exporter-4.49.2
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: obs
    name: obs-prometheus-node-exporter-h74z2
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: obs-prometheus-node-exporter
      uid: 58785a52-3ec3-4b27-a5e3-723c4b122405
    resourceVersion: "741390"
    uid: b00c893a-92bd-4fd4-9369-aeada2e33069
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - aks-nodepool1-41991914-vmss000000
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: aks-nodepool1-41991914-vmss000000
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: obs-prometheus-node-exporter
    serviceAccountName: obs-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:54Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fad17073dd751b341ba90963e8c4b603a47c5d2038fc68c3e2ca024db27955dc
      image: quay.io/prometheus/node-exporter:v1.10.2
      imageID: quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565
      lastState: {}
      name: node-exporter
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:11:53Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.224.0.4
    hostIPs:
    - ip: 10.224.0.4
    phase: Running
    podIP: 10.224.0.4
    podIPs:
    - ip: 10.224.0.4
    qosClass: BestEffort
    startTime: "2025-12-27T11:11:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      opentelemetry-operator-config/sha256: 97b2cf8cab91ec2cd285c39894688d53c2a0b3a155d04adeed5fb55ae2c22b20
      prometheus.io/path: /metrics
      prometheus.io/port: "8888"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-27T13:45:59Z"
    generateName: otel-collector-collector-bb45c95f7-
    generation: 1
    labels:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: monitoring.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: otel-collector-collector
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: latest
      pod-template-hash: bb45c95f7
    name: otel-collector-collector-bb45c95f7-hf7x9
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: otel-collector-collector-bb45c95f7
      uid: 8ff64bc7-f3e8-4122-bde6-4eea003350d3
    resourceVersion: "797914"
    uid: e2a2902e-f315-4504-90e9-f6e426806627
  spec:
    containers:
    - args:
      - --config=/conf/collector.yaml
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: GOMEMLIMIT
        valueFrom:
          resourceFieldRef:
            containerName: otc-container
            divisor: "0"
            resource: limits.memory
      - name: GOMAXPROCS
        valueFrom:
          resourceFieldRef:
            containerName: otc-container
            divisor: "0"
            resource: limits.cpu
      image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector:0.141.0
      imagePullPolicy: IfNotPresent
      name: otc-container
      ports:
      - containerPort: 8888
        name: metrics
        protocol: TCP
      - containerPort: 4317
        name: otlp-grpc
        protocol: TCP
      - containerPort: 4318
        name: otlp-http
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /conf
        name: otc-internal
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rzvg6
        readOnly: true
    dnsConfig: {}
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: aks-nodepool1-41991914-vmss000000
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: otel-collector-collector
    serviceAccountName: otel-collector-collector
    shareProcessNamespace: false
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: collector.yaml
          path: collector.yaml
        name: otel-collector-collector-97b2cf8c
      name: otc-internal
    - name: kube-api-access-rzvg6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T13:46:00Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T13:45:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T13:46:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T13:46:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T13:45:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://db01efa184f05fe8cf310dd242cd16a4fce29ee08460620431f75e80a4995bc7
      image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector:0.141.0
      imageID: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector@sha256:b54b114431b2aa031c28f590507b4a17daabd59a4d797eb7fddd7988b4d3e3ea
      lastState: {}
      name: otc-container
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T13:45:59Z"
      volumeMounts:
      - mountPath: /conf
        name: otc-internal
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rzvg6
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.224.0.4
    hostIPs:
    - ip: 10.224.0.4
    phase: Running
    podIP: 10.244.0.134
    podIPs:
    - ip: 10.244.0.134
    qosClass: BestEffort
    startTime: "2025-12-27T13:45:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
    creationTimestamp: "2025-12-27T11:11:55Z"
    generateName: prometheus-obs-kube-prometheus-stack-prometheus-
    generation: 1
    labels:
      app.kubernetes.io/instance: obs-kube-prometheus-stack-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 3.8.1
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-obs-kube-prometheus-stack-prometheus-6455c4cd6
      operator.prometheus.io/name: obs-kube-prometheus-stack-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: obs-kube-prometheus-stack-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-obs-kube-prometheus-stack-prometheus-0
    name: prometheus-obs-kube-prometheus-stack-prometheus-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-obs-kube-prometheus-stack-prometheus
      uid: 843dff1a-cd9a-455f-b1d6-4806e855b24f
    resourceVersion: "741816"
    uid: f18d3619-d1b4-4207-b32a-c4f896431db6
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - prometheus
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - obs-kube-prometheus-stack-prometheus
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --web.enable-lifecycle
      - --web.external-url=http://obs-kube-prometheus-stack-prometheus.monitoring:9090
      - --web.route-prefix=/
      - --storage.tsdb.retention.time=10d
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.wal-compression
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v3.8.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-obs-kube-prometheus-stack-prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
        readOnly: true
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
        readOnly: true
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        readOnly: true
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gntv8
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
      - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
      - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gntv8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-obs-kube-prometheus-stack-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
      - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
      - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-init
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gntv8
        readOnly: true
    nodeName: aks-nodepool1-41991914-vmss000001
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: obs-kube-prometheus-stack-prometheus
    serviceAccountName: obs-kube-prometheus-stack-prometheus
    shareProcessNamespace: false
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-obs-kube-prometheus-stack-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-obs-kube-prometheus-stack-prometheus-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
        optional: true
      name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
    - configMap:
        defaultMode: 420
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
        optional: true
      name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
    - configMap:
        defaultMode: 420
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        optional: true
      name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-obs-kube-prometheus-stack-prometheus-web-config
    - emptyDir: {}
      name: prometheus-obs-kube-prometheus-stack-prometheus-db
    - name: kube-api-access-gntv8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:12:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:12:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:12:41Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:12:41Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:11:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a5e1cfc3736d6d6ff57607dae7b7aee5e0c888cb7db64df4d4b55cd8816c10c4
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:257ce3543e8f45c059dab488bcf897d8cc598cabd78afba12f41d6dbe36e22d2
      lastState: {}
      name: config-reloader
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:12:37Z"
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gntv8
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://be4580f900e19b87bd3d9839a72c61f7146efe6f016882f8d728ed00e5c277cf
      image: quay.io/prometheus/prometheus:v3.8.1
      imageID: quay.io/prometheus/prometheus@sha256:2b6f734e372c1b4717008f7d0a0152316aedd4d13ae17ef1e3268dbfaf68041b
      lastState: {}
      name: prometheus
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:12:36Z"
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /prometheus
        name: prometheus-obs-kube-prometheus-stack-prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gntv8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.224.0.5
    hostIPs:
    - ip: 10.224.0.5
    initContainerStatuses:
    - containerID: containerd://2a95a8bc22431c4db05e024356687c3ab35dd1067c2187f9566b6bb95336fc61
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:257ce3543e8f45c059dab488bcf897d8cc598cabd78afba12f41d6dbe36e22d2
      lastState: {}
      name: init-config-reloader
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://2a95a8bc22431c4db05e024356687c3ab35dd1067c2187f9566b6bb95336fc61
          exitCode: 0
          finishedAt: "2025-12-27T11:12:03Z"
          reason: Completed
          startedAt: "2025-12-27T11:12:03Z"
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
      - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gntv8
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.244.1.60
    podIPs:
    - ip: 10.244.1.60
    qosClass: BestEffort
    startTime: "2025-12-27T11:11:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: a7d9a7697e773f51140e63950219d06036cdfa9e40fb393115b30aae3be908e1
    creationTimestamp: "2025-12-27T11:59:47Z"
    generateName: tempo-
    generation: 1
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: tempo-74d74bf86f
      statefulset.kubernetes.io/pod-name: tempo-0
    name: tempo-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: tempo
      uid: 02c7fcd9-35cf-4928-9937-b6af3135b02a
    resourceVersion: "758932"
    uid: e4eeab59-71f6-419e-9191-c58237d5666f
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/conf/tempo.yaml
      - -mem-ballast-size-mbs=1024
      image: docker.io/grafana/tempo:2.9.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 3200
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: tempo
      ports:
      - containerPort: 3200
        name: prom-metrics
        protocol: TCP
      - containerPort: 6831
        name: jaeger-thrift-c
        protocol: UDP
      - containerPort: 6832
        name: jaeger-thrift-b
        protocol: UDP
      - containerPort: 14268
        name: jaeger-thrift-h
        protocol: TCP
      - containerPort: 14250
        name: jaeger-grpc
        protocol: TCP
      - containerPort: 9411
        name: zipkin
        protocol: TCP
      - containerPort: 55680
        name: otlp-legacy
        protocol: TCP
      - containerPort: 4317
        name: otlp-grpc
        protocol: TCP
      - containerPort: 55681
        name: otlp-httplegacy
        protocol: TCP
      - containerPort: 4318
        name: otlp-http
        protocol: TCP
      - containerPort: 55678
        name: opencensus
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 3200
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /conf
        name: tempo-conf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jbdwc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: tempo-0
    nodeName: aks-nodepool1-41991914-vmss000001
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: tempo
    serviceAccountName: tempo
    subdomain: tempo-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: tempo
      name: tempo-conf
    - name: kube-api-access-jbdwc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:59:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:59:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T12:00:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T12:00:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T11:59:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cbeff890084d85a45889d983d9334df650bf8f5e457822bcf475a25a564f4b00
      image: docker.io/grafana/tempo:2.9.0
      imageID: docker.io/grafana/tempo@sha256:65a5789759435f1ef696f1953258b9bbdb18eb571d5ce711ff812d2e128288a4
      lastState: {}
      name: tempo
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-27T11:59:51Z"
      volumeMounts:
      - mountPath: /conf
        name: tempo-conf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jbdwc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.224.0.5
    hostIPs:
    - ip: 10.224.0.5
    phase: Running
    podIP: 10.244.1.58
    podIPs:
    - ip: 10.244.1.58
    qosClass: BestEffort
    startTime: "2025-12-27T11:59:47Z"
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-27T11:11:55Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      managed-by: prometheus-operator
      operated-alertmanager: "true"
    name: alertmanager-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: obs-kube-prometheus-stack-alertmanager
      uid: 0e1260a3-3273-443d-a3e8-77cae10d14c9
    resourceVersion: "741418"
    uid: 4005baa4-c967-4f96-94bd-21dcd5438c76
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: http-web
    - name: tcp-mesh
      port: 9094
      protocol: TCP
      targetPort: mesh-tcp
    - name: udp-mesh
      port: 9094
      protocol: UDP
      targetPort: mesh-udp
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-27T11:41:34Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.3.0
      helm.sh/chart: grafana-10.4.0
    name: obs-grafana
    namespace: monitoring
    resourceVersion: "751806"
    uid: c95164c7-25b2-4543-bfcb-8ba63d6f3627
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.0.103.194
    clusterIPs:
    - 10.0.103.194
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 30377
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: obs
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 172.210.122.215
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.7.0
      chart: kube-prometheus-stack-80.7.0
      heritage: Helm
      release: obs
      self-monitor: "true"
    name: obs-kube-prometheus-stack-alertmanager
    namespace: monitoring
    resourceVersion: "741238"
    uid: 8b85a236-6e18-4551-b23a-aa90530635b8
  spec:
    clusterIP: 10.0.174.199
    clusterIPs:
    - 10.0.174.199
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: 9093
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      alertmanager: obs-kube-prometheus-stack-alertmanager
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.7.0
      chart: kube-prometheus-stack-80.7.0
      heritage: Helm
      release: obs
    name: obs-kube-prometheus-stack-operator
    namespace: monitoring
    resourceVersion: "741242"
    uid: edf069c6-225c-4262-b036-26d39ad506e9
  spec:
    clusterIP: 10.0.56.160
    clusterIPs:
    - 10.0.56.160
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app: kube-prometheus-stack-operator
      release: obs
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.7.0
      chart: kube-prometheus-stack-80.7.0
      heritage: Helm
      release: obs
      self-monitor: "true"
    name: obs-kube-prometheus-stack-prometheus
    namespace: monitoring
    resourceVersion: "744749"
    uid: d4246a1f-8156-4ca9-9162-63da90f93bac
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.0.146.4
    clusterIPs:
    - 10.0.146.4
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      nodePort: 32575
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      nodePort: 32168
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/name: prometheus
      operator.prometheus.io/name: obs-kube-prometheus-stack-prometheus
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 4.255.55.128
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-7.0.0
      release: obs
    name: obs-kube-state-metrics
    namespace: monitoring
    resourceVersion: "741216"
    uid: 42f30261-6889-4986-976d-c5cf4ce8657b
  spec:
    clusterIP: 10.0.190.254
    clusterIPs:
    - 10.0.190.254
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: obs
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-27T11:11:50Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      helm.sh/chart: prometheus-node-exporter-4.49.2
      jobLabel: node-exporter
      release: obs
    name: obs-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "741222"
    uid: b169fc0e-7dd7-42f4-9e8c-67185775d9b1
  spec:
    clusterIP: 10.0.23.42
    clusterIPs:
    - 10.0.23.42
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: obs
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-27T13:45:59Z"
    labels:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: monitoring.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: otel-collector-collector
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: latest
      operator.opentelemetry.io/collector-service-type: base
    name: otel-collector-collector
    namespace: monitoring
    ownerReferences:
    - apiVersion: opentelemetry.io/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: OpenTelemetryCollector
      name: otel-collector
      uid: beada918-dc8d-4655-85c3-1d9fe0d77879
    resourceVersion: "797874"
    uid: b9c0be6b-69f0-43e0-8bd7-a5030f09b06d
  spec:
    clusterIP: 10.0.46.42
    clusterIPs:
    - 10.0.46.42
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: grpc
      name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - appProtocol: http
      name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    selector:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: monitoring.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/part-of: opentelemetry
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      service.beta.openshift.io/serving-cert-secret-name: otel-collector-collector-headless-tls
    creationTimestamp: "2025-12-27T13:45:59Z"
    labels:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: monitoring.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: otel-collector-collector
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: latest
      operator.opentelemetry.io/collector-headless-service: Exists
      operator.opentelemetry.io/collector-service-type: headless
    name: otel-collector-collector-headless
    namespace: monitoring
    ownerReferences:
    - apiVersion: opentelemetry.io/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: OpenTelemetryCollector
      name: otel-collector
      uid: beada918-dc8d-4655-85c3-1d9fe0d77879
    resourceVersion: "797877"
    uid: 5e81a790-c470-4833-ad7e-4a7d27b77f6f
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: grpc
      name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - appProtocol: http
      name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    selector:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: monitoring.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/part-of: opentelemetry
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-27T13:45:59Z"
    labels:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: monitoring.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: otel-collector-collector-monitoring
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: latest
      operator.opentelemetry.io/collector-monitoring-service: Exists
      operator.opentelemetry.io/collector-service-type: monitoring
    name: otel-collector-collector-monitoring
    namespace: monitoring
    ownerReferences:
    - apiVersion: opentelemetry.io/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: OpenTelemetryCollector
      name: otel-collector
      uid: beada918-dc8d-4655-85c3-1d9fe0d77879
    resourceVersion: "797890"
    uid: ed219cdc-1d26-4a02-b281-e7a9b8e17908
  spec:
    clusterIP: 10.0.147.77
    clusterIPs:
    - 10.0.147.77
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: monitoring
      port: 8888
      protocol: TCP
      targetPort: 8888
    selector:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: monitoring.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/part-of: opentelemetry
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-27T11:11:55Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      managed-by: prometheus-operator
      operated-prometheus: "true"
    name: prometheus-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: obs-kube-prometheus-stack-prometheus
      uid: 3552afec-6ab4-45c6-9fe0-acf52932b48e
    resourceVersion: "741453"
    uid: f9be74eb-568c-4336-b791-edd560bc8100
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: http-web
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: tempo
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:59:46Z"
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: tempo
      app.kubernetes.io/version: 2.9.0
      helm.sh/chart: tempo-1.24.1
    name: tempo
    namespace: monitoring
    resourceVersion: "758631"
    uid: b6b2926d-6d27-4706-a665-4a979f7fc807
  spec:
    clusterIP: 10.0.186.169
    clusterIPs:
    - 10.0.186.169
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tempo-jaeger-thrift-compact
      port: 6831
      protocol: UDP
      targetPort: 6831
    - name: tempo-jaeger-thrift-binary
      port: 6832
      protocol: UDP
      targetPort: 6832
    - name: tempo-prom-metrics
      port: 3200
      protocol: TCP
      targetPort: 3200
    - name: tempo-jaeger-thrift-http
      port: 14268
      protocol: TCP
      targetPort: 14268
    - name: grpc-tempo-jaeger
      port: 14250
      protocol: TCP
      targetPort: 14250
    - name: tempo-zipkin
      port: 9411
      protocol: TCP
      targetPort: 9411
    - name: tempo-otlp-legacy
      port: 55680
      protocol: TCP
      targetPort: 55680
    - name: tempo-otlp-http-legacy
      port: 55681
      protocol: TCP
      targetPort: 55681
    - name: grpc-tempo-otlp
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: tempo-otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: tempo-opencensus
      port: 55678
      protocol: TCP
      targetPort: 55678
    selector:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      helm.sh/chart: prometheus-node-exporter-4.49.2
      release: obs
    name: obs-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "741393"
    uid: 58785a52-3ec3-4b27-a5e3-723c4b122405
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: obs
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: obs
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.10.2
          helm.sh/chart: prometheus-node-exporter-4.49.2
          jobLabel: node-exporter
          release: obs
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                  - fargate
                - key: type
                  operator: NotIn
                  values:
                  - virtual-kubelet
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
          - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: obs-prometheus-node-exporter
        serviceAccountName: obs-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.3.0
      helm.sh/chart: grafana-10.4.0
    name: obs-grafana
    namespace: monitoring
    resourceVersion: "742005"
    uid: bfd15daf-6004-4537-927e-8353b30518ca
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: obs
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 481413ea19473fd6217e3a5bcbd4fe4215ddf5da919e91a3e2d32b793cbd8f8d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: obs
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.3.0
          helm.sh/chart: grafana-10.4.0
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: obs-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: obs-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          - name: REQ_SKIP_INIT
            value: "true"
          image: quay.io/kiwigrid/k8s-sidecar:2.1.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: obs-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: obs-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          - name: REQ_SKIP_INIT
            value: "true"
          image: quay.io/kiwigrid/k8s-sidecar:2.1.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: obs-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: obs-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:12.3.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: obs-grafana
        serviceAccountName: obs-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: obs-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: obs-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-27T11:13:10Z"
      lastUpdateTime: "2025-12-27T11:13:10Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-27T11:11:50Z"
      lastUpdateTime: "2025-12-27T11:13:10Z"
      message: ReplicaSet "obs-grafana-6d4d847f7c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.7.0
      chart: kube-prometheus-stack-80.7.0
      heritage: Helm
      release: obs
    name: obs-kube-prometheus-stack-operator
    namespace: monitoring
    resourceVersion: "741450"
    uid: 14bcf8db-4b89-44aa-b2a1-3ac64049372a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        release: obs
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: obs
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 80.7.0
          chart: kube-prometheus-stack-80.7.0
          heritage: Helm
          release: obs
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/obs-kube-prometheus-stack-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.40.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.87.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: obs-kube-prometheus-stack-operator
        serviceAccountName: obs-kube-prometheus-stack-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: obs-kube-prometheus-stack-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-27T11:11:55Z"
      lastUpdateTime: "2025-12-27T11:11:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-27T11:11:50Z"
      lastUpdateTime: "2025-12-27T11:11:55Z"
      message: ReplicaSet "obs-kube-prometheus-stack-operator-74bb6546d6" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-7.0.0
      release: obs
    name: obs-kube-state-metrics
    namespace: monitoring
    resourceVersion: "741591"
    uid: a9125778-0dcf-4f54-9354-e111aa2d7fe1
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: obs
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: obs
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.17.0
          helm.sh/chart: kube-state-metrics-7.0.0
          release: obs
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpointslices,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: obs-kube-state-metrics
        serviceAccountName: obs-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-27T11:12:07Z"
      lastUpdateTime: "2025-12-27T11:12:07Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-27T11:11:50Z"
      lastUpdateTime: "2025-12-27T11:12:07Z"
      message: ReplicaSet "obs-kube-state-metrics-6df98f8558" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-12-27T13:45:59Z"
    generation: 1
    labels:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: monitoring.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: otel-collector-collector
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: latest
    name: otel-collector-collector
    namespace: monitoring
    ownerReferences:
    - apiVersion: opentelemetry.io/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: OpenTelemetryCollector
      name: otel-collector
      uid: beada918-dc8d-4655-85c3-1d9fe0d77879
    resourceVersion: "797923"
    uid: a4c59b84-41fc-4858-9f4f-e8759befe691
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: opentelemetry-collector
        app.kubernetes.io/instance: monitoring.otel-collector
        app.kubernetes.io/managed-by: opentelemetry-operator
        app.kubernetes.io/part-of: opentelemetry
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          opentelemetry-operator-config/sha256: 97b2cf8cab91ec2cd285c39894688d53c2a0b3a155d04adeed5fb55ae2c22b20
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: opentelemetry-collector
          app.kubernetes.io/instance: monitoring.otel-collector
          app.kubernetes.io/managed-by: opentelemetry-operator
          app.kubernetes.io/name: otel-collector-collector
          app.kubernetes.io/part-of: opentelemetry
          app.kubernetes.io/version: latest
      spec:
        containers:
        - args:
          - --config=/conf/collector.yaml
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                containerName: otc-container
                divisor: "0"
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                containerName: otc-container
                divisor: "0"
                resource: limits.cpu
          image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector:0.141.0
          imagePullPolicy: IfNotPresent
          name: otc-container
          ports:
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: otc-internal
        dnsConfig: {}
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: otel-collector-collector
        serviceAccountName: otel-collector-collector
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: collector.yaml
              path: collector.yaml
            name: otel-collector-collector-97b2cf8c
          name: otc-internal
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-27T13:46:00Z"
      lastUpdateTime: "2025-12-27T13:46:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-27T13:45:59Z"
      lastUpdateTime: "2025-12-27T13:46:00Z"
      message: ReplicaSet "otel-collector-collector-bb45c95f7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: obs
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.3.0
      helm.sh/chart: grafana-10.4.0
      pod-template-hash: 6d4d847f7c
    name: obs-grafana-6d4d847f7c
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: obs-grafana
      uid: bfd15daf-6004-4537-927e-8353b30518ca
    resourceVersion: "742004"
    uid: e99c8ca5-4970-492c-874a-cde1c0b5f1b0
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: obs
        app.kubernetes.io/name: grafana
        pod-template-hash: 6d4d847f7c
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 481413ea19473fd6217e3a5bcbd4fe4215ddf5da919e91a3e2d32b793cbd8f8d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: obs
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.3.0
          helm.sh/chart: grafana-10.4.0
          pod-template-hash: 6d4d847f7c
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: obs-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: obs-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          - name: REQ_SKIP_INIT
            value: "true"
          image: quay.io/kiwigrid/k8s-sidecar:2.1.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: obs-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: obs-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          - name: REQ_SKIP_INIT
            value: "true"
          image: quay.io/kiwigrid/k8s-sidecar:2.1.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: obs-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: obs-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:12.3.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: obs-grafana
        serviceAccountName: obs-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: obs-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: obs-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.7.0
      chart: kube-prometheus-stack-80.7.0
      heritage: Helm
      pod-template-hash: 74bb6546d6
      release: obs
    name: obs-kube-prometheus-stack-operator-74bb6546d6
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: obs-kube-prometheus-stack-operator
      uid: 14bcf8db-4b89-44aa-b2a1-3ac64049372a
    resourceVersion: "741446"
    uid: 0d33c79d-2863-497f-9869-b502b4e6ce81
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: 74bb6546d6
        release: obs
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: obs
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 80.7.0
          chart: kube-prometheus-stack-80.7.0
          heritage: Helm
          pod-template-hash: 74bb6546d6
          release: obs
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/obs-kube-prometheus-stack-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.40.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.87.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: obs-kube-prometheus-stack-operator
        serviceAccountName: obs-kube-prometheus-stack-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: obs-kube-prometheus-stack-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:11:50Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: obs
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-7.0.0
      pod-template-hash: 6df98f8558
      release: obs
    name: obs-kube-state-metrics-6df98f8558
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: obs-kube-state-metrics
      uid: a9125778-0dcf-4f54-9354-e111aa2d7fe1
    resourceVersion: "741589"
    uid: b00b5089-ae0d-4173-8381-fb380c513d38
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: obs
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 6df98f8558
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: obs
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.17.0
          helm.sh/chart: kube-state-metrics-7.0.0
          pod-template-hash: 6df98f8558
          release: obs
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpointslices,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: obs-kube-state-metrics
        serviceAccountName: obs-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-12-27T13:45:59Z"
    generation: 1
    labels:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: monitoring.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: otel-collector-collector
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: latest
      pod-template-hash: bb45c95f7
    name: otel-collector-collector-bb45c95f7
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-collector
      uid: a4c59b84-41fc-4858-9f4f-e8759befe691
    resourceVersion: "797922"
    uid: 8ff64bc7-f3e8-4122-bde6-4eea003350d3
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: opentelemetry-collector
        app.kubernetes.io/instance: monitoring.otel-collector
        app.kubernetes.io/managed-by: opentelemetry-operator
        app.kubernetes.io/part-of: opentelemetry
        pod-template-hash: bb45c95f7
    template:
      metadata:
        annotations:
          opentelemetry-operator-config/sha256: 97b2cf8cab91ec2cd285c39894688d53c2a0b3a155d04adeed5fb55ae2c22b20
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: opentelemetry-collector
          app.kubernetes.io/instance: monitoring.otel-collector
          app.kubernetes.io/managed-by: opentelemetry-operator
          app.kubernetes.io/name: otel-collector-collector
          app.kubernetes.io/part-of: opentelemetry
          app.kubernetes.io/version: latest
          pod-template-hash: bb45c95f7
      spec:
        containers:
        - args:
          - --config=/conf/collector.yaml
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                containerName: otc-container
                divisor: "0"
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                containerName: otc-container
                divisor: "0"
                resource: limits.cpu
          image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector:0.141.0
          imagePullPolicy: IfNotPresent
          name: otc-container
          ports:
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: otc-internal
        dnsConfig: {}
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: otel-collector-collector
        serviceAccountName: otel-collector-collector
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: collector.yaml
              path: collector.yaml
            name: otel-collector-collector-97b2cf8c
          name: otc-internal
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "1970821483891944606"
    creationTimestamp: "2025-12-27T11:11:55Z"
    generation: 1
    labels:
      alertmanager: obs-kube-prometheus-stack-alertmanager
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: obs-kube-prometheus-stack-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.7.0
      chart: kube-prometheus-stack-80.7.0
      heritage: Helm
      managed-by: prometheus-operator
      release: obs
    name: alertmanager-obs-kube-prometheus-stack-alertmanager
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Alertmanager
      name: obs-kube-prometheus-stack-alertmanager
      uid: 0e1260a3-3273-443d-a3e8-77cae10d14c9
    resourceVersion: "741553"
    uid: 685cd12b-b2f3-43d4-a02f-689122153bba
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        alertmanager: obs-kube-prometheus-stack-alertmanager
        app.kubernetes.io/instance: obs-kube-prometheus-stack-alertmanager
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: alertmanager
    serviceName: alertmanager-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: alertmanager
        creationTimestamp: null
        labels:
          alertmanager: obs-kube-prometheus-stack-alertmanager
          app.kubernetes.io/instance: obs-kube-prometheus-stack-alertmanager
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: alertmanager
          app.kubernetes.io/version: 0.30.0
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - alertmanager
                  - key: alertmanager
                    operator: In
                    values:
                    - obs-kube-prometheus-stack-alertmanager
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --storage.path=/alertmanager
          - --data.retention=120h
          - --cluster.listen-address=
          - --web.listen-address=:9093
          - --web.external-url=http://obs-kube-prometheus-stack-alertmanager.monitoring:9093
          - --web.route-prefix=/
          - --cluster.label=monitoring/obs-kube-prometheus-stack-alertmanager
          - --cluster.peer=alertmanager-obs-kube-prometheus-stack-alertmanager-0.alertmanager-operated:9094
          - --cluster.reconnect-timeout=5m
          - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.30.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http-web
            protocol: TCP
          - containerPort: 9094
            name: mesh-tcp
            protocol: TCP
          - containerPort: 9094
            name: mesh-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
          - mountPath: /etc/alertmanager/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/alertmanager/certs
            name: tls-assets
            readOnly: true
          - mountPath: /alertmanager
            name: alertmanager-obs-kube-prometheus-stack-alertmanager-db
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
            name: cluster-tls-config
            readOnly: true
            subPath: cluster-tls-config.yaml
        - args:
          - --listen-address=:8080
          - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
          - --reload-url=http://127.0.0.1:9093/-/reload
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-init
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: obs-kube-prometheus-stack-alertmanager
        serviceAccountName: obs-kube-prometheus-stack-alertmanager
        terminationGracePeriodSeconds: 120
        volumes:
        - name: config-volume
          secret:
            defaultMode: 420
            secretName: alertmanager-obs-kube-prometheus-stack-alertmanager-generated
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: alertmanager-obs-kube-prometheus-stack-alertmanager-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - name: web-config
          secret:
            defaultMode: 420
            secretName: alertmanager-obs-kube-prometheus-stack-alertmanager-web-config
        - name: cluster-tls-config
          secret:
            defaultMode: 420
            secretName: alertmanager-obs-kube-prometheus-stack-alertmanager-cluster-tls-config
        - emptyDir: {}
          name: alertmanager-obs-kube-prometheus-stack-alertmanager-db
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: alertmanager-obs-kube-prometheus-stack-alertmanager-5b5bf7994b
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: alertmanager-obs-kube-prometheus-stack-alertmanager-5b5bf7994b
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: obs
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "4009077480873453536"
    creationTimestamp: "2025-12-27T11:11:55Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: obs-kube-prometheus-stack-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.7.0
      chart: kube-prometheus-stack-80.7.0
      heritage: Helm
      managed-by: prometheus-operator
      operator.prometheus.io/mode: server
      operator.prometheus.io/name: obs-kube-prometheus-stack-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: obs-kube-prometheus-stack-prometheus
      release: obs
    name: prometheus-obs-kube-prometheus-stack-prometheus
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Prometheus
      name: obs-kube-prometheus-stack-prometheus
      uid: 3552afec-6ab4-45c6-9fe0-acf52932b48e
    resourceVersion: "741821"
    uid: 843dff1a-cd9a-455f-b1d6-4806e855b24f
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: obs-kube-prometheus-stack-prometheus
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: prometheus
        operator.prometheus.io/name: obs-kube-prometheus-stack-prometheus
        operator.prometheus.io/shard: "0"
        prometheus: obs-kube-prometheus-stack-prometheus
    serviceName: prometheus-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: obs-kube-prometheus-stack-prometheus
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/version: 3.8.1
          operator.prometheus.io/name: obs-kube-prometheus-stack-prometheus
          operator.prometheus.io/shard: "0"
          prometheus: obs-kube-prometheus-stack-prometheus
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - prometheus
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                    - obs-kube-prometheus-stack-prometheus
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
          - --web.enable-lifecycle
          - --web.external-url=http://obs-kube-prometheus-stack-prometheus.monitoring:9090
          - --web.route-prefix=/
          - --storage.tsdb.retention.time=10d
          - --storage.tsdb.path=/prometheus
          - --storage.tsdb.wal-compression
          - --web.config.file=/etc/prometheus/web_config/web-config.yaml
          image: quay.io/prometheus/prometheus:v3.8.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: prometheus
          ports:
          - containerPort: 9090
            name: http-web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/prometheus/certs
            name: tls-assets
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-obs-kube-prometheus-stack-prometheus-db
          - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
            readOnly: true
          - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
            readOnly: true
          - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
            readOnly: true
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
          - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
          - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
          - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
          - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
          - --watched-dir=/etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-init
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
          - mountPath: /etc/prometheus/rules/prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: obs-kube-prometheus-stack-prometheus
        serviceAccountName: obs-kube-prometheus-stack-prometheus
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 600
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: prometheus-obs-kube-prometheus-stack-prometheus
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: prometheus-obs-kube-prometheus-stack-prometheus-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - configMap:
            defaultMode: 420
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
            optional: true
          name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-0
        - configMap:
            defaultMode: 420
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
            optional: true
          name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-1
        - configMap:
            defaultMode: 420
            name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
            optional: true
          name: prometheus-obs-kube-prometheus-stack-prometheus-rulefiles-2
        - name: web-config
          secret:
            defaultMode: 420
            secretName: prometheus-obs-kube-prometheus-stack-prometheus-web-config
        - emptyDir: {}
          name: prometheus-obs-kube-prometheus-stack-prometheus-db
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-obs-kube-prometheus-stack-prometheus-6455c4cd6
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-obs-kube-prometheus-stack-prometheus-6455c4cd6
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: tempo
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-12-27T11:59:46Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: tempo
      app.kubernetes.io/version: 2.9.0
      helm.sh/chart: tempo-1.24.1
    name: tempo
    namespace: monitoring
    resourceVersion: "758935"
    uid: 02c7fcd9-35cf-4928-9937-b6af3135b02a
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: tempo
        app.kubernetes.io/name: tempo
    serviceName: tempo-headless
    template:
      metadata:
        annotations:
          checksum/config: a7d9a7697e773f51140e63950219d06036cdfa9e40fb393115b30aae3be908e1
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: tempo
          app.kubernetes.io/name: tempo
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/conf/tempo.yaml
          - -mem-ballast-size-mbs=1024
          image: docker.io/grafana/tempo:2.9.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 3200
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: tempo
          ports:
          - containerPort: 3200
            name: prom-metrics
            protocol: TCP
          - containerPort: 6831
            name: jaeger-thrift-c
            protocol: UDP
          - containerPort: 6832
            name: jaeger-thrift-b
            protocol: UDP
          - containerPort: 14268
            name: jaeger-thrift-h
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55680
            name: otlp-legacy
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 55681
            name: otlp-httplegacy
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 55678
            name: opencensus
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 3200
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: tempo-conf
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: tempo
        serviceAccountName: tempo
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: tempo
          name: tempo-conf
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: tempo-74d74bf86f
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: tempo-74d74bf86f
    updatedReplicas: 1
kind: List
metadata:
  resourceVersion: ""
